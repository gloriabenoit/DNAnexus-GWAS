{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Overview This tutorial provides a step-by-step guide on the use of DNAnexus to perform a GWAS on the UK Biobank data. We will run everything from the command line, but will monitor our jobs on the project's web page. This tutorial assumes that you already have a researcher account on the UKBiobank site, and have access to a DNAnexus project. If not, please create an account , and once it has been validated, create an account on UKB-RAP with your UK Biobank credentials to access DNAnexus. To setup your first project, please check the official documentation on the matter . This tutorial will guide you through every step needed in order to perform a basic GWAS using whole genome sequences for chromosome 1 to 22 for both PLINK2 and regenie . In order to parallelize the analyses, we will perform 22 different GWAS, one for each chromosome, and combine the results locally to reduce cost. PLINK2 is the most basic and simple tool to perform a GWAS, while regenie is a bit more advanced. If you need to choose only one, please perform the PLINK2 tutorial , as it is simpler, quicker and way cheaper to complete. As an example, we will perform a linear regression on the BMI index ( 21001 ) using whole genome sequencing data , specifically the DRAGEN WGS 500k release ( /Bulk/DRAGEN WGS/ , data release v19.1 ) and the GraphTyper WGS 200k release ( /Bulk/Previous WGS releases/GATK and GraphTyper WGS/ , data release v15.1 ). However, you can use any data that you need, simply keep in mind that paths need to be changed in the scripts, and execution time will vary. This tutorial is written for Linux operating systems. Commands may vary accross operating systems. The official \"GWAS guide using Alzheimer's disease\" and the official github page for a \"GWAS on the Research Analysis Platform using regenie\" were useful material when writing the regenie section . If you want to try other tools on DNAnexus, we recommend the following github page: ukb-rap-tools by Phil Greer . Structure The tutorial is separated into four main sections: Fondamentals for first-time use of DNAnexus ( Getting started , About jobs ) Necessary files for GWAS input ( Input files ) Running a GWAS ( Using PLINK2 , Using regenie ) Generating plots from results ( Visualizing results ) Please note, the PLINK2 and regenie sections are independant of each other and can be done separately. However, you will need to follow first the Getting started and Input files pages to make sure you have everything necessary to their completion. Requirements To follow this tutorial, you will only need Bash and Python 3 . Total cost Please note, when first using your account you have an initial credit of \u00a340. Running all of this tutorial with the same instance and priority as us should not go over this budget. However, if it is not done already, your project should be billed to a wallet which is different from your initial credit. Please check the official documentation if you are unsure on how to proceed. Jobs cost will be computed based on the official UKB RAP Rate Card (v3.0). Please remember the cost and duration of a job depends on the instance and priority used. Execution time may also vary for the same instance. Final architecture At the end of this tutorial, your DNAnexus project's architecture should look like this: \u251c\u2500\u2500 Bulk/ \u251c\u2500\u2500 Showcase metadata/ \u251c\u2500\u2500 gwas_tutorial/ \u2502 \u251c\u2500\u2500 plink_gwas_BMI/ \u2502 \u2502 \u251c\u2500\u2500 sumstat_c1.BMI.glm.linear \u2502 \u2502 \u251c\u2500\u2500 ... \u2502 \u2502 \u2514\u2500\u2500 sumstat_c22.BMI.glm.linear \u2502 \u251c\u2500\u2500 reg_gwas_BMI/ \u2502 \u2502 \u251c\u2500\u2500 merge/ \u2502 \u2502 \u251c\u2500\u2500 QC_lists/ \u2502 \u2502 \u251c\u2500\u2500 sumstat_c1_BMI.regenie.gz \u2502 \u2502 \u251c\u2500\u2500 ... \u2502 \u2502 \u2514\u2500\u2500 sumstat_c22_BMI.regenie.gz \u2502 \u251c\u2500\u2500 BMI.txt \u2502 \u251c\u2500\u2500 covariates.txt \u2502 \u2514\u2500\u2500 white_british.txt \u251c\u2500\u2500 app-id \u2514\u2500\u2500 app-id.dataset This will vary based on whether you use both PLINK2 and regenie, or only one, and if you have changed files/repertory names. Please note, if modified, files and repertory names have to be the same across all commands.","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#overview","text":"This tutorial provides a step-by-step guide on the use of DNAnexus to perform a GWAS on the UK Biobank data. We will run everything from the command line, but will monitor our jobs on the project's web page. This tutorial assumes that you already have a researcher account on the UKBiobank site, and have access to a DNAnexus project. If not, please create an account , and once it has been validated, create an account on UKB-RAP with your UK Biobank credentials to access DNAnexus. To setup your first project, please check the official documentation on the matter . This tutorial will guide you through every step needed in order to perform a basic GWAS using whole genome sequences for chromosome 1 to 22 for both PLINK2 and regenie . In order to parallelize the analyses, we will perform 22 different GWAS, one for each chromosome, and combine the results locally to reduce cost. PLINK2 is the most basic and simple tool to perform a GWAS, while regenie is a bit more advanced. If you need to choose only one, please perform the PLINK2 tutorial , as it is simpler, quicker and way cheaper to complete. As an example, we will perform a linear regression on the BMI index ( 21001 ) using whole genome sequencing data , specifically the DRAGEN WGS 500k release ( /Bulk/DRAGEN WGS/ , data release v19.1 ) and the GraphTyper WGS 200k release ( /Bulk/Previous WGS releases/GATK and GraphTyper WGS/ , data release v15.1 ). However, you can use any data that you need, simply keep in mind that paths need to be changed in the scripts, and execution time will vary. This tutorial is written for Linux operating systems. Commands may vary accross operating systems. The official \"GWAS guide using Alzheimer's disease\" and the official github page for a \"GWAS on the Research Analysis Platform using regenie\" were useful material when writing the regenie section . If you want to try other tools on DNAnexus, we recommend the following github page: ukb-rap-tools by Phil Greer .","title":"Overview"},{"location":"#structure","text":"The tutorial is separated into four main sections: Fondamentals for first-time use of DNAnexus ( Getting started , About jobs ) Necessary files for GWAS input ( Input files ) Running a GWAS ( Using PLINK2 , Using regenie ) Generating plots from results ( Visualizing results ) Please note, the PLINK2 and regenie sections are independant of each other and can be done separately. However, you will need to follow first the Getting started and Input files pages to make sure you have everything necessary to their completion.","title":"Structure"},{"location":"#requirements","text":"To follow this tutorial, you will only need Bash and Python 3 .","title":"Requirements"},{"location":"#total-cost","text":"Please note, when first using your account you have an initial credit of \u00a340. Running all of this tutorial with the same instance and priority as us should not go over this budget. However, if it is not done already, your project should be billed to a wallet which is different from your initial credit. Please check the official documentation if you are unsure on how to proceed. Jobs cost will be computed based on the official UKB RAP Rate Card (v3.0). Please remember the cost and duration of a job depends on the instance and priority used. Execution time may also vary for the same instance.","title":"Total cost"},{"location":"#final-architecture","text":"At the end of this tutorial, your DNAnexus project's architecture should look like this: \u251c\u2500\u2500 Bulk/ \u251c\u2500\u2500 Showcase metadata/ \u251c\u2500\u2500 gwas_tutorial/ \u2502 \u251c\u2500\u2500 plink_gwas_BMI/ \u2502 \u2502 \u251c\u2500\u2500 sumstat_c1.BMI.glm.linear \u2502 \u2502 \u251c\u2500\u2500 ... \u2502 \u2502 \u2514\u2500\u2500 sumstat_c22.BMI.glm.linear \u2502 \u251c\u2500\u2500 reg_gwas_BMI/ \u2502 \u2502 \u251c\u2500\u2500 merge/ \u2502 \u2502 \u251c\u2500\u2500 QC_lists/ \u2502 \u2502 \u251c\u2500\u2500 sumstat_c1_BMI.regenie.gz \u2502 \u2502 \u251c\u2500\u2500 ... \u2502 \u2502 \u2514\u2500\u2500 sumstat_c22_BMI.regenie.gz \u2502 \u251c\u2500\u2500 BMI.txt \u2502 \u251c\u2500\u2500 covariates.txt \u2502 \u2514\u2500\u2500 white_british.txt \u251c\u2500\u2500 app-id \u2514\u2500\u2500 app-id.dataset This will vary based on whether you use both PLINK2 and regenie, or only one, and if you have changed files/repertory names. Please note, if modified, files and repertory names have to be the same across all commands.","title":"Final architecture"},{"location":"input/","text":"Input files This section is done locally and does not cost anything. Please be aware that storing files onto DNAnexus will result in a monthly cost. You may check its current value in the SETTINGS tab on your project's web page, or compute it using the UKB RAP Rate Card . Genetic data On DNAnexus, large items, like genetic data, are stored in the Bulk folder at the root of your project. Among them, you can find whole exome sequencing data ( /Bulk/Exome sequences/ ) or whole genome sequencing data ( /Bulk/DRAGEN WGS/ or /Bulk/Previous WGS releases/ ). Whole genome sequencing data is available in multiple formats: BGEN format ( .bgen and .sample ), PLINK format ( .bed , .bim and .fam ), PLINK2 format ( .pgen , .psam and .pvar ) or pVCF format ( .vcf.gz.tbi ). Feel free to browse the Bulk folder for more information on the data available. As written on the About jobs page , input files can be either downloaded or mounted to the worker. We have tested both to see which one was quicker. PLINK2 After some trial and error, we have found that downloading the input in the PLINK2 format was the quickest way to perform a GWAS with PLINK2. This makes sense seeing as the PLINK2 format is innate to PLINK2 and the most compressed of all available formats, but we were surprised to found that mounting was slower. The path to the genetic data chosen is the following: /Bulk/DRAGEN WGS/DRAGEN population level WGS variants, PLINK format [500k release]/ . Please be aware, when working on previous releases, we had found that using the BGEN format without mounting it was quicker than downloading or mounting the PLINK files. regenie We first wanted to use the DRAGEN 500k data for regenie as well, but that has proven to be quite difficult for a number of reasons. Namely, as of writing this tutorial, the DRAGEN 500k .psam file does not have a header, which results in ERROR: header does not have the correct format. when using regenie ( Issue about this error ). Therefore, we cannot use the PLINK2 data. regenie's Step 2 is optimized for input genotype data in BGEN v1.2 format. However, the BGEN data for DRAGEN 500k is very large: the file for chromosome 1 is bigger than a tebibyte (1.01 TiB). Thus, higher instances are needed, costs add up and execution time is increased (since PLINK2 is used and has to translate the format). Therefore, we choose to use previous releases, which are way less heavy (chromosome 1 is 71.49 GiB). Regarding the mount or download of the data, there seems to be an error when mounting data to regenie ( Corresponding issue ). Because of this, we confirm the download the input, which should take less time and will garantee complete results. The path to the genetic data chosen is the following: /Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]/ . Please be aware, we have not tested all possibilities, and specific formats might work better with specific data. Therefore, if you choose to use any other data, you might want to check which is better. Additional data In order to run our GWAS, apart from the genetic and sample data, we need 3 additionnal files: The phenotype (for instance, BMI) The ids of individuals we wish to keep (for instance, white british) The covariates to use (for instance, 16 genetic principal components, the sex and the age) This tutorial will guide you through the local creation of these files using DNAnexus data, and their upload to your project using dx upload . All three files will be uploaded to your current dx repertory ( gwas_tutorial if you are closely following this tutorial). If you want to upload them somewhere else, you can either move directory with dx cd before uploading, or use the --path or --destination option to specify the DNAnexus path to upload the files. Phenotype To extract the phenotype that we want, we first need to download all available data-fields in our dataset. The files downloaded will have an incredibly long name, we want to rename it to a more manageable name like ukbb . The rename command is a built-in tool on many Linux distributions. However, if you don't have it, you can install it with sudo apt install rename . dataset=$(dx ls /app*.dataset --brief) dx extract_dataset $dataset -ddd --delimiter \",\" dataset_name=$(basename *.data_dictionary.csv | sed -e \"s/.data_dictionary.csv//\") rename $dataset_name ukbb $dataset_name* This command outputs 3 files: ukbb.data_dictionary.csv (6.7 Mo) contains a table with participants as the rows and metadata along the columns, including field names (see table below for naming convention) ukbb.codings.csv (6.2 Mo) contains a lookup table for the different medical codes, including ICD-10 codes that will be displayed in the diagnosis field column (p41270) ukbb.entity_dictionary.csv (1.9 Ko) contains the different entities that can be queried, where participant is the main entity that corresponds to most phenotype fields Please note, when running the extract_dataset command you might encounter a ('Invalid JSON received from server', 200) error. If this happens, you simply need to rerun the code. We first need to get the field name of the phenotype(s) we want to extract. As an example, we will extract the BMI index ( 21001 ), but you can extract any number of phenotypes. For the main participant phenotype entity, the Research Analysis Platform (UKB-RAP) uses field names with the following convention: Type of field Syntax for field name Example Neither instanced nor arrayed p<FIELD> p31 Instanced but not arrayed p<FIELD>_i<INSTANCE> p40005_i0 Arrayed but not instanced p<FIELD>_a<ARRAY> p41262_a0 Instanced and arrayed p<FIELD>_i<INSTANCE>_a<ARRAY> p93_i0_a0 This means one phenotype ID can actually have multiple data field. For example, BMI has four instances. The following python script will first extract every array or instance associated to your phenotype ( field_names_for_ids() ), then keep only the first one for each ( select_first() ). We choose to keep only the first instance since multiple columns will be analyzed as separate phenotypes. Please note, the file computed is temporary, it is not necessary to rename it since it's not the final phenotype file. Moreover, since extract_dataset has no overwrite option by design, we implemented it ourselves. Running the following code will first delete pheno_extract.csv if it's present, allowing for the extraction to happen. \"\"\" Extract phenotype(s) from UKBB based on field ID(s). \"\"\" import os import re import subprocess import dxpy import pandas as pd def field_names_for_ids(filename, field_ids): \"\"\" Convert data-field id to corresponding field name. Parameters ---------- filename : str Path to the '.dataset.data_dictionary.csv' file. field_ids : list All field ids. data.frame (pandas) Projects's .data_dictionary.csv file. Returns ------- list All corresponding field names. \"\"\" data = pd.read_csv(filename, sep=',') field_names = [\"eid\"] for _id in field_ids: select = list(data[data.name.str.match(r'^p{}(_i\\d+)?(_a\\d+)?$'.format(_id))].name.values) field_names += select field_names = [f\"participant.{f}\" for f in field_names] return field_names def select_first(field_names): \"\"\"Select only first data for a field. Parameters ---------- field_names : list All corresponding field names. Returns ------- list All corresponding field names.\"\"\" selection = [] for field in field_names: # Not instanced nor arrayed if not re.search(\"_\", field): selection.append(field) else: f_inst = re.search(\"_i\", field) f_arr = re.search(\"_a\", field) f_inst_0 = re.search(\"i0\", field) f_arr_0 = re.search(\"a0\", field) # Instanced and arrayed if f_inst_0 and f_arr_0: selection.append(field) continue # Instanced not arrayed if f_inst_0 and not f_arr: selection.append(field) # Arrayed not instanced if f_arr_0 and not f_inst: selection.append(field) return selection # Input DATASET = dxpy.find_one_data_object(typename='Dataset', name='app*.dataset', folder='/', name_mode='glob')['id'] FILENAME = \"ukbb.data_dictionary.csv\" OUTPUT = \"pheno_extract.csv\" FIELD_ID = [21001] # BMI id # Convert id to names FIELD_NAMES = field_names_for_ids(FILENAME, FIELD_ID) FIELD_NAMES = select_first(FIELD_NAMES) FIELD_NAMES = \",\".join(FIELD_NAMES) # Extract phenotype(s) if os.path.exists(OUTPUT): os.remove(OUTPUT) cmd = [\"dx\", \"extract_dataset\", DATASET, \"--fields\", FIELD_NAMES, \"--delimiter\", \",\", \"--output\", OUTPUT] subprocess.check_call(cmd) This command outputs 1 file: pheno_extract.csv (8.0 Mo) contains the values for participant IDs and the first instance of BMI values. Please be aware, extracting a huge quantity of phenotypes may consistently run into a ('Invalid JSON received from server', 200) error. If so, we suggest extracting multiple files, and combining them into one. You can use the following command to do so: paste -d',' file1.csv file2.csv > merged.csv PLINK2 and regenie use the same formatting for the phenotype file, thus we can use the same file for both. We first need to duplicate the individuals ID, mark missing values as 'NA' (which is recognized by both regenie and PLINK2 with the necessary options) and name our phenotype. More information on phenotype files formatting can be found here for PLINK2 and here for regenie. \"\"\" Format phenotype file. \"\"\" import pandas as pd # Input FILENAME = \"pheno_extract.csv\" PHENOTYPE = \"BMI\" def format_phenotype(filename, phenotype): \"\"\" Save phenotype to correct format. Parameters ---------- filename : str Phenotype file to format. phenotype : str Phenotype name. \"\"\" output = f\"{phenotype}.txt\" # Format file data = pd.read_csv(filename, sep=',', skiprows=1, names=[\"IID\", phenotype]) fid = data[\"IID\"].rename(\"FID\") merged = pd.concat([fid, data], axis=1) merged.to_csv(output, sep='\\t', index=False, header=True, na_rep='NA') # Save phenotype format_phenotype(FILENAME, PHENOTYPE) This command outputs 1 file: BMI.txt (11.42 MiB) contains the formatted phenotype values You can now upload the formated phenotype file. dx upload BMI.txt Individual ids Running a GWAS on a specific population helps reduce the bias caused by population stratification. It is therefore an important step. To filter out individuals based on their ethnic background, we can use the phenotype extraction script and the data-field 21000 . You simply need to change the value of FIELD_ID : # FIELD_ID = [21001] # BMI id FIELD_ID = [21000] # ethnic background id This field uses the 1001 data encoding . In this code, 1001 means white british which is the main ethnic group, and the one we want to select. pop_code=1001 pop_name=\"white_british\" population=\"pheno_extract.csv\" awk -F \",\" -v var=\"$pop_code\" '$2~var{print $1,$1}' $population > $pop_name.txt This command outputs 1 file: white_british.txt (6.75 MiB) contains the participants IDs of the white british ethnic background You can now upload the ids of your individuals. dx upload white_british.txt Covariates The genetic principal components from the UKBB individuals are stored in the 22009 data field. We could use our phenotype extraction script , but it appears to constantly end in a BadJSONInReply error . We have found a community post by Alex Shemy which seems to encounter the same problem as us. However, it highlights that not everyone encounters this error, and our script might work on your machine. In any case, we provide the following script which will extract exactly what we want and bypass the error. We will use 18 variables as covariates: the first 16 PCA components, the sex (data field 31 ) and the age (data field 21003 ) of individuals. You can extract whatever you want as covariates. However, we have chosen to keep only the first 16 PCA components, following a study by Priv\u00e9 et al. in 2020 which indicates other components capture complex LD structure rather than population structure. More information on covariates files formatting can be found here for PLINK2 and here for regenie. dataset=$(dx ls /app*.dataset --brief) field_names=\"participant.eid,participant.eid,\" for i in {1..16} do field_names+=\"participant.p22009_a$i,\" done field_names+=\"participant.p31,participant.p21003_i0\" # Sex and age dx extract_dataset $dataset --fields $field_names --delimiter \",\" --output covariates.txt echo -e \"FID,IID,PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,PC11,PC12,PC13,PC14,PC15,PC16,Sex,Age\" > file.tmp tail -n+2 covariates.txt >> file.tmp awk -F , '$3!=\"\"' file.tmp > covariates.txt # Remove ind with no PC data sed 's/,/\\t/g' covariates.txt > file.tmp mv file.tmp covariates.txt This command outputs 1 file: covariates.txt (74.59 MiB) contains the first 16 PCA components, the sex and the age of all participants You can now upload the covariates file. dx upload covariates.txt","title":"Input files"},{"location":"input/#input-files","text":"This section is done locally and does not cost anything. Please be aware that storing files onto DNAnexus will result in a monthly cost. You may check its current value in the SETTINGS tab on your project's web page, or compute it using the UKB RAP Rate Card .","title":"Input files"},{"location":"input/#genetic-data","text":"On DNAnexus, large items, like genetic data, are stored in the Bulk folder at the root of your project. Among them, you can find whole exome sequencing data ( /Bulk/Exome sequences/ ) or whole genome sequencing data ( /Bulk/DRAGEN WGS/ or /Bulk/Previous WGS releases/ ). Whole genome sequencing data is available in multiple formats: BGEN format ( .bgen and .sample ), PLINK format ( .bed , .bim and .fam ), PLINK2 format ( .pgen , .psam and .pvar ) or pVCF format ( .vcf.gz.tbi ). Feel free to browse the Bulk folder for more information on the data available. As written on the About jobs page , input files can be either downloaded or mounted to the worker. We have tested both to see which one was quicker.","title":"Genetic data"},{"location":"input/#plink2","text":"After some trial and error, we have found that downloading the input in the PLINK2 format was the quickest way to perform a GWAS with PLINK2. This makes sense seeing as the PLINK2 format is innate to PLINK2 and the most compressed of all available formats, but we were surprised to found that mounting was slower. The path to the genetic data chosen is the following: /Bulk/DRAGEN WGS/DRAGEN population level WGS variants, PLINK format [500k release]/ . Please be aware, when working on previous releases, we had found that using the BGEN format without mounting it was quicker than downloading or mounting the PLINK files.","title":"PLINK2"},{"location":"input/#regenie","text":"We first wanted to use the DRAGEN 500k data for regenie as well, but that has proven to be quite difficult for a number of reasons. Namely, as of writing this tutorial, the DRAGEN 500k .psam file does not have a header, which results in ERROR: header does not have the correct format. when using regenie ( Issue about this error ). Therefore, we cannot use the PLINK2 data. regenie's Step 2 is optimized for input genotype data in BGEN v1.2 format. However, the BGEN data for DRAGEN 500k is very large: the file for chromosome 1 is bigger than a tebibyte (1.01 TiB). Thus, higher instances are needed, costs add up and execution time is increased (since PLINK2 is used and has to translate the format). Therefore, we choose to use previous releases, which are way less heavy (chromosome 1 is 71.49 GiB). Regarding the mount or download of the data, there seems to be an error when mounting data to regenie ( Corresponding issue ). Because of this, we confirm the download the input, which should take less time and will garantee complete results. The path to the genetic data chosen is the following: /Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]/ . Please be aware, we have not tested all possibilities, and specific formats might work better with specific data. Therefore, if you choose to use any other data, you might want to check which is better.","title":"regenie"},{"location":"input/#additional-data","text":"In order to run our GWAS, apart from the genetic and sample data, we need 3 additionnal files: The phenotype (for instance, BMI) The ids of individuals we wish to keep (for instance, white british) The covariates to use (for instance, 16 genetic principal components, the sex and the age) This tutorial will guide you through the local creation of these files using DNAnexus data, and their upload to your project using dx upload . All three files will be uploaded to your current dx repertory ( gwas_tutorial if you are closely following this tutorial). If you want to upload them somewhere else, you can either move directory with dx cd before uploading, or use the --path or --destination option to specify the DNAnexus path to upload the files.","title":"Additional data"},{"location":"input/#phenotype","text":"To extract the phenotype that we want, we first need to download all available data-fields in our dataset. The files downloaded will have an incredibly long name, we want to rename it to a more manageable name like ukbb . The rename command is a built-in tool on many Linux distributions. However, if you don't have it, you can install it with sudo apt install rename . dataset=$(dx ls /app*.dataset --brief) dx extract_dataset $dataset -ddd --delimiter \",\" dataset_name=$(basename *.data_dictionary.csv | sed -e \"s/.data_dictionary.csv//\") rename $dataset_name ukbb $dataset_name* This command outputs 3 files: ukbb.data_dictionary.csv (6.7 Mo) contains a table with participants as the rows and metadata along the columns, including field names (see table below for naming convention) ukbb.codings.csv (6.2 Mo) contains a lookup table for the different medical codes, including ICD-10 codes that will be displayed in the diagnosis field column (p41270) ukbb.entity_dictionary.csv (1.9 Ko) contains the different entities that can be queried, where participant is the main entity that corresponds to most phenotype fields Please note, when running the extract_dataset command you might encounter a ('Invalid JSON received from server', 200) error. If this happens, you simply need to rerun the code. We first need to get the field name of the phenotype(s) we want to extract. As an example, we will extract the BMI index ( 21001 ), but you can extract any number of phenotypes. For the main participant phenotype entity, the Research Analysis Platform (UKB-RAP) uses field names with the following convention: Type of field Syntax for field name Example Neither instanced nor arrayed p<FIELD> p31 Instanced but not arrayed p<FIELD>_i<INSTANCE> p40005_i0 Arrayed but not instanced p<FIELD>_a<ARRAY> p41262_a0 Instanced and arrayed p<FIELD>_i<INSTANCE>_a<ARRAY> p93_i0_a0 This means one phenotype ID can actually have multiple data field. For example, BMI has four instances. The following python script will first extract every array or instance associated to your phenotype ( field_names_for_ids() ), then keep only the first one for each ( select_first() ). We choose to keep only the first instance since multiple columns will be analyzed as separate phenotypes. Please note, the file computed is temporary, it is not necessary to rename it since it's not the final phenotype file. Moreover, since extract_dataset has no overwrite option by design, we implemented it ourselves. Running the following code will first delete pheno_extract.csv if it's present, allowing for the extraction to happen. \"\"\" Extract phenotype(s) from UKBB based on field ID(s). \"\"\" import os import re import subprocess import dxpy import pandas as pd def field_names_for_ids(filename, field_ids): \"\"\" Convert data-field id to corresponding field name. Parameters ---------- filename : str Path to the '.dataset.data_dictionary.csv' file. field_ids : list All field ids. data.frame (pandas) Projects's .data_dictionary.csv file. Returns ------- list All corresponding field names. \"\"\" data = pd.read_csv(filename, sep=',') field_names = [\"eid\"] for _id in field_ids: select = list(data[data.name.str.match(r'^p{}(_i\\d+)?(_a\\d+)?$'.format(_id))].name.values) field_names += select field_names = [f\"participant.{f}\" for f in field_names] return field_names def select_first(field_names): \"\"\"Select only first data for a field. Parameters ---------- field_names : list All corresponding field names. Returns ------- list All corresponding field names.\"\"\" selection = [] for field in field_names: # Not instanced nor arrayed if not re.search(\"_\", field): selection.append(field) else: f_inst = re.search(\"_i\", field) f_arr = re.search(\"_a\", field) f_inst_0 = re.search(\"i0\", field) f_arr_0 = re.search(\"a0\", field) # Instanced and arrayed if f_inst_0 and f_arr_0: selection.append(field) continue # Instanced not arrayed if f_inst_0 and not f_arr: selection.append(field) # Arrayed not instanced if f_arr_0 and not f_inst: selection.append(field) return selection # Input DATASET = dxpy.find_one_data_object(typename='Dataset', name='app*.dataset', folder='/', name_mode='glob')['id'] FILENAME = \"ukbb.data_dictionary.csv\" OUTPUT = \"pheno_extract.csv\" FIELD_ID = [21001] # BMI id # Convert id to names FIELD_NAMES = field_names_for_ids(FILENAME, FIELD_ID) FIELD_NAMES = select_first(FIELD_NAMES) FIELD_NAMES = \",\".join(FIELD_NAMES) # Extract phenotype(s) if os.path.exists(OUTPUT): os.remove(OUTPUT) cmd = [\"dx\", \"extract_dataset\", DATASET, \"--fields\", FIELD_NAMES, \"--delimiter\", \",\", \"--output\", OUTPUT] subprocess.check_call(cmd) This command outputs 1 file: pheno_extract.csv (8.0 Mo) contains the values for participant IDs and the first instance of BMI values. Please be aware, extracting a huge quantity of phenotypes may consistently run into a ('Invalid JSON received from server', 200) error. If so, we suggest extracting multiple files, and combining them into one. You can use the following command to do so: paste -d',' file1.csv file2.csv > merged.csv PLINK2 and regenie use the same formatting for the phenotype file, thus we can use the same file for both. We first need to duplicate the individuals ID, mark missing values as 'NA' (which is recognized by both regenie and PLINK2 with the necessary options) and name our phenotype. More information on phenotype files formatting can be found here for PLINK2 and here for regenie. \"\"\" Format phenotype file. \"\"\" import pandas as pd # Input FILENAME = \"pheno_extract.csv\" PHENOTYPE = \"BMI\" def format_phenotype(filename, phenotype): \"\"\" Save phenotype to correct format. Parameters ---------- filename : str Phenotype file to format. phenotype : str Phenotype name. \"\"\" output = f\"{phenotype}.txt\" # Format file data = pd.read_csv(filename, sep=',', skiprows=1, names=[\"IID\", phenotype]) fid = data[\"IID\"].rename(\"FID\") merged = pd.concat([fid, data], axis=1) merged.to_csv(output, sep='\\t', index=False, header=True, na_rep='NA') # Save phenotype format_phenotype(FILENAME, PHENOTYPE) This command outputs 1 file: BMI.txt (11.42 MiB) contains the formatted phenotype values You can now upload the formated phenotype file. dx upload BMI.txt","title":"Phenotype"},{"location":"input/#individual-ids","text":"Running a GWAS on a specific population helps reduce the bias caused by population stratification. It is therefore an important step. To filter out individuals based on their ethnic background, we can use the phenotype extraction script and the data-field 21000 . You simply need to change the value of FIELD_ID : # FIELD_ID = [21001] # BMI id FIELD_ID = [21000] # ethnic background id This field uses the 1001 data encoding . In this code, 1001 means white british which is the main ethnic group, and the one we want to select. pop_code=1001 pop_name=\"white_british\" population=\"pheno_extract.csv\" awk -F \",\" -v var=\"$pop_code\" '$2~var{print $1,$1}' $population > $pop_name.txt This command outputs 1 file: white_british.txt (6.75 MiB) contains the participants IDs of the white british ethnic background You can now upload the ids of your individuals. dx upload white_british.txt","title":"Individual ids"},{"location":"input/#covariates","text":"The genetic principal components from the UKBB individuals are stored in the 22009 data field. We could use our phenotype extraction script , but it appears to constantly end in a BadJSONInReply error . We have found a community post by Alex Shemy which seems to encounter the same problem as us. However, it highlights that not everyone encounters this error, and our script might work on your machine. In any case, we provide the following script which will extract exactly what we want and bypass the error. We will use 18 variables as covariates: the first 16 PCA components, the sex (data field 31 ) and the age (data field 21003 ) of individuals. You can extract whatever you want as covariates. However, we have chosen to keep only the first 16 PCA components, following a study by Priv\u00e9 et al. in 2020 which indicates other components capture complex LD structure rather than population structure. More information on covariates files formatting can be found here for PLINK2 and here for regenie. dataset=$(dx ls /app*.dataset --brief) field_names=\"participant.eid,participant.eid,\" for i in {1..16} do field_names+=\"participant.p22009_a$i,\" done field_names+=\"participant.p31,participant.p21003_i0\" # Sex and age dx extract_dataset $dataset --fields $field_names --delimiter \",\" --output covariates.txt echo -e \"FID,IID,PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,PC11,PC12,PC13,PC14,PC15,PC16,Sex,Age\" > file.tmp tail -n+2 covariates.txt >> file.tmp awk -F , '$3!=\"\"' file.tmp > covariates.txt # Remove ind with no PC data sed 's/,/\\t/g' covariates.txt > file.tmp mv file.tmp covariates.txt This command outputs 1 file: covariates.txt (74.59 MiB) contains the first 16 PCA components, the sex and the age of all participants You can now upload the covariates file. dx upload covariates.txt","title":"Covariates"},{"location":"jobs/","text":"About jobs If this is your first time using DNAnexus, or if you are unsure about how some things work, please read this page. However, be aware that it is not mandatory to complete the tutorial. For more detailed information, you can check the official documentation for a quickstart or about the key concepts of the platform. Execution On DNAnexus, when a job is executed, a worker is spun up in the cloud, then the job's code and inputs are downloaded to that worker and executed. This is the standard behavior, but inputs can also be mounted (accessed dynamically) to the worker to avoid their downloading costs. Inputs Since jobs are run on workers, you need to manually specify every file used as input. To add a file as input, you need to use the -iin option to your dx run command. The path to your file needs to be absolute so that there is no confusion as to where it is located. When mounting your files, you don't need to input them manually anymore. You simply need to add /mnt/project/ before your absolute path in the code. Instances To choose an instance for your job is to choose a specific worker on which your code will be executed. Instances have 3 main metrics: The number of cores (individual processing units) The memory capacity (components used for short-term data retention) The storage capacity (components used for long-term data retention) You might have no clue on what ressources your job needs to run properly, the simplest course of action is to start with a small instance and increase its capacities if you encounter any errors. Please check the Errors when running section if you don't know how to interpret your error message. Please note, this tutorial uses the same instance for every job. Depending on the data you want to use, you might need a bigger instance. If your goal is full optimization, then you might prefer a smaller one. All of this is up to you. To specify the instance you chose, you need to add the --instance-type option to your dx run command. For more information on instances, check the official documentation on instance types . Instances costs can be found on the UKB RAP Rate Card . Priority Jobs can have three level of priority: high means your job will be completed once the ressources are available normal means your job will wait for 15min to run in a low priority, and if no ressources are availables by then, will run in a high priority low means your job will start once the ressources are available, but may be interrupted if high priority jobs need them Although a high priority assures for job completion, it is also pricier than a low priority one (around x3 or x5). Please note, this tutorial will only run low priority jobs to reduce costs. To specify the level you chose, you need to add the --priority option to your dx run command. For more information on priorities, check the official documentation on job priority . Priority costs can be found on the UKB RAP Rate Card (where high refers to On-demand prices while low refers to Spot prices). Tags Tags, although optional, are very pratical when using DNAnexus regularly. They act as keywords associated to a job, and help if you need to navigate the MONITOR tab. You can find more information in the Monitoring section . To add a tag to your job, you need to add the --tag option to your dx run command. Time and cost limits By default, a job will result in an error if its execution time exceeds 30 days. Please note that this value may vary across apps. During this tutorial, no jobs will exceed this limit. Setting a cost limit is optional, but can be useful. For instance, when running a low priority job, it can ensure that possible interruptions won't add up causing the cost to rise by stopping the job early. It may also be of use if you don't know the time your job will take, and therefore can't compute the total cost. Either way, it is useful to avoid spending too much on a single job. To specify a cost limit, you need to add the --cost-limit option to your dx run command. For more information on limits, check the official documentation on time limits or on cost and spending limits . Monitoring You can monitor your jobs directly on your project's web page in the MONITOR tab. On this page, they are sorted in ascending order of launch date (the latest comes first). Basic information about the job is displayed, like its name, duration, cost, etc ... A job has 3 possible states: Done indicates that the job has completed successfully Waiting indicates that the ressources needed to run this job are not available yet, it will start once they are Failed indicates the job has failed due to an error Once a job is completed (either Done or Failed ), you will receive an email updating you of said completion. It will also appear in the Notifications tab (bell icon in the top right corner). Filtering jobs After running a lot of jobs, you might find your MONITOR page to be quite crowded. This is especially true with this tutorial where some jobs are run for each of the 22 chromosomes. To find a specific job, you can filter out the MONITOR page using specific values like the state or the ID of the user who launched the execution, for instance. In this tutorial, all jobs have specific tags which help with filtering. By default, tags are not used in filtering on the MONITOR page, you need to add them in the Filter settings tab (three stacked bars in the top right corner). Please note, in this tutorial, jobs are tagged with the software used (plink or regenie), the phenotype (BMI), the step of the analysis (QC, GWAS, Merge, Step 1 and 2) and lastly the chromosome number (c1 to c22) if jobs are separated per chromosome. Errors when running A failed job running on DNAnexus has 3 main errors (apart from errors in the code): Warning: Out of memory error occurred during this job. Warning: Low disk space during this job. The machine running the job was terminated by the cloud provider This tutorial will explain the reason for those errors, and what you can do to avoid them. Out of memory If the cause of failure is the following: Error while running the command (please refer to the job log for more information). Warning: Out of memory error occurred during this job. You need to choose an instance with a bigger memory. The memory infix in the instance is mem . For more information on instances, refer to the Instances section . Low disk space If the cause of failure is the following: Error while running the command (please refer to the job log for more information). Warning: Low disk space during this job. You need to choose an instance with a bigger storage. The storage infix in the instance is ssd or hdd . For more information on instances, refer to the Instances section . Interruption limit (only for low priority) Jobs with a low priority can be interrupted if their ressources are needed by high priority jobs. A low priority job can be interrupted at most 10 times, then it will result in the following error: The machine running the job was terminated by the cloud provider You simply need to restart your job, either on the same instance or another one, if the one used is too popular. Others If the error is unclear to you, check the log of your job. You will most likely get an answer there.","title":"About jobs"},{"location":"jobs/#about-jobs","text":"If this is your first time using DNAnexus, or if you are unsure about how some things work, please read this page. However, be aware that it is not mandatory to complete the tutorial. For more detailed information, you can check the official documentation for a quickstart or about the key concepts of the platform.","title":"About jobs"},{"location":"jobs/#execution","text":"On DNAnexus, when a job is executed, a worker is spun up in the cloud, then the job's code and inputs are downloaded to that worker and executed. This is the standard behavior, but inputs can also be mounted (accessed dynamically) to the worker to avoid their downloading costs.","title":"Execution"},{"location":"jobs/#inputs","text":"Since jobs are run on workers, you need to manually specify every file used as input. To add a file as input, you need to use the -iin option to your dx run command. The path to your file needs to be absolute so that there is no confusion as to where it is located. When mounting your files, you don't need to input them manually anymore. You simply need to add /mnt/project/ before your absolute path in the code.","title":"Inputs"},{"location":"jobs/#instances","text":"To choose an instance for your job is to choose a specific worker on which your code will be executed. Instances have 3 main metrics: The number of cores (individual processing units) The memory capacity (components used for short-term data retention) The storage capacity (components used for long-term data retention) You might have no clue on what ressources your job needs to run properly, the simplest course of action is to start with a small instance and increase its capacities if you encounter any errors. Please check the Errors when running section if you don't know how to interpret your error message. Please note, this tutorial uses the same instance for every job. Depending on the data you want to use, you might need a bigger instance. If your goal is full optimization, then you might prefer a smaller one. All of this is up to you. To specify the instance you chose, you need to add the --instance-type option to your dx run command. For more information on instances, check the official documentation on instance types . Instances costs can be found on the UKB RAP Rate Card .","title":"Instances"},{"location":"jobs/#priority","text":"Jobs can have three level of priority: high means your job will be completed once the ressources are available normal means your job will wait for 15min to run in a low priority, and if no ressources are availables by then, will run in a high priority low means your job will start once the ressources are available, but may be interrupted if high priority jobs need them Although a high priority assures for job completion, it is also pricier than a low priority one (around x3 or x5). Please note, this tutorial will only run low priority jobs to reduce costs. To specify the level you chose, you need to add the --priority option to your dx run command. For more information on priorities, check the official documentation on job priority . Priority costs can be found on the UKB RAP Rate Card (where high refers to On-demand prices while low refers to Spot prices).","title":"Priority"},{"location":"jobs/#tags","text":"Tags, although optional, are very pratical when using DNAnexus regularly. They act as keywords associated to a job, and help if you need to navigate the MONITOR tab. You can find more information in the Monitoring section . To add a tag to your job, you need to add the --tag option to your dx run command.","title":"Tags"},{"location":"jobs/#time-and-cost-limits","text":"By default, a job will result in an error if its execution time exceeds 30 days. Please note that this value may vary across apps. During this tutorial, no jobs will exceed this limit. Setting a cost limit is optional, but can be useful. For instance, when running a low priority job, it can ensure that possible interruptions won't add up causing the cost to rise by stopping the job early. It may also be of use if you don't know the time your job will take, and therefore can't compute the total cost. Either way, it is useful to avoid spending too much on a single job. To specify a cost limit, you need to add the --cost-limit option to your dx run command. For more information on limits, check the official documentation on time limits or on cost and spending limits .","title":"Time and cost limits"},{"location":"jobs/#monitoring","text":"You can monitor your jobs directly on your project's web page in the MONITOR tab. On this page, they are sorted in ascending order of launch date (the latest comes first). Basic information about the job is displayed, like its name, duration, cost, etc ... A job has 3 possible states: Done indicates that the job has completed successfully Waiting indicates that the ressources needed to run this job are not available yet, it will start once they are Failed indicates the job has failed due to an error Once a job is completed (either Done or Failed ), you will receive an email updating you of said completion. It will also appear in the Notifications tab (bell icon in the top right corner).","title":"Monitoring"},{"location":"jobs/#filtering-jobs","text":"After running a lot of jobs, you might find your MONITOR page to be quite crowded. This is especially true with this tutorial where some jobs are run for each of the 22 chromosomes. To find a specific job, you can filter out the MONITOR page using specific values like the state or the ID of the user who launched the execution, for instance. In this tutorial, all jobs have specific tags which help with filtering. By default, tags are not used in filtering on the MONITOR page, you need to add them in the Filter settings tab (three stacked bars in the top right corner). Please note, in this tutorial, jobs are tagged with the software used (plink or regenie), the phenotype (BMI), the step of the analysis (QC, GWAS, Merge, Step 1 and 2) and lastly the chromosome number (c1 to c22) if jobs are separated per chromosome.","title":"Filtering jobs"},{"location":"jobs/#errors-when-running","text":"A failed job running on DNAnexus has 3 main errors (apart from errors in the code): Warning: Out of memory error occurred during this job. Warning: Low disk space during this job. The machine running the job was terminated by the cloud provider This tutorial will explain the reason for those errors, and what you can do to avoid them.","title":"Errors when running"},{"location":"jobs/#out-of-memory","text":"If the cause of failure is the following: Error while running the command (please refer to the job log for more information). Warning: Out of memory error occurred during this job. You need to choose an instance with a bigger memory. The memory infix in the instance is mem . For more information on instances, refer to the Instances section .","title":"Out of memory"},{"location":"jobs/#low-disk-space","text":"If the cause of failure is the following: Error while running the command (please refer to the job log for more information). Warning: Low disk space during this job. You need to choose an instance with a bigger storage. The storage infix in the instance is ssd or hdd . For more information on instances, refer to the Instances section .","title":"Low disk space"},{"location":"jobs/#interruption-limit-only-for-low-priority","text":"Jobs with a low priority can be interrupted if their ressources are needed by high priority jobs. A low priority job can be interrupted at most 10 times, then it will result in the following error: The machine running the job was terminated by the cloud provider You simply need to restart your job, either on the same instance or another one, if the one used is too popular.","title":"Interruption limit (only for low priority)"},{"location":"jobs/#others","text":"If the error is unclear to you, check the log of your job. You will most likely get an answer there.","title":"Others"},{"location":"plink/","text":"Using PLINK2 Every file created during this analysis will be stored in a main directory called plink_gwas_BMI . This section will run 22 different jobs (one per chromosome). With our chosen instance (mem1_ssd1_v2_x16) using a high priority, the whole GWAS will take about 40 minutes and cost around \u00a33.84 (for a total execution time of 528 minutes). With the same instance (mem1_ssd1_v2_x16) using a low priority, if no jobs are interrupted, it will cost only \u00a31.03 for the same time. Please note that it is most unlikely for jobs to be uninterrupted. In our experience, when accounting for interrupted/failed jobs, the total cost is around one to two times higher than the low cost, but still lower than the high cost. Although please keep in mind that the total time needed can be two to five times higher, depending on the interrupted/failed jobs. Input files The path to the genetic data chosen is the following: /Bulk/DRAGEN WGS/DRAGEN population level WGS variants, PLINK format [500k release]/ . Before running a GWAS on DNAnexus using PLINK2, you need to make sure you have these 3 files uploaded to DNAnexus: The phenotype: BMI.txt The ids of individuals we wish to keep: white_british.txt The covariates to use: covariates.txt You can check their presence with the following command: dx ls Please refer to the Input files section if you don't have these files. Running a GWAS On DNAnexus, PLINK2 is available either as part of the Swiss Army Knife app ( swiss-army-knife ) or as its own app called plink_gwas . We will use the former. We choose to use the same instance for all GWASs, to simplify the code, but this can be changed to your liking. Same for the priority and the cost limit. Quality control We perform the QC at the same time as our GWAS. The variants are filtered using the following options: --maf 0.01 --hwe 1e-50 --geno 0.1 Please change the thresholds according to your preferences, but be aware any change will modify jobs execution time, cost and size. Please note that we choose not to use the --mind filter, although it is a conventional, as it removes 88% of the samples for chromosome 18. However, we have not checked for this behavior in other data format so it might be specific to the PLINK2 format. Linear regression Following a study by Rivas and Chang in 2025 , we will add the qt-residualize modifier to the --glm option, which performs a single regression on the covariates upfront, and then passes the phenotype residuals. In our experience, this divides the execution time by 3 while yielding the same results as --glm alone, which is why we choose to use it. It also has the added bonus that it scales greatly when analyzing multiple phenotypes (keeping the same execution time for more than a hundred phenotypes). Please be aware that this option may have limits. We recommend making sure that the results are expected. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" ind_path=\"/gwas_tutorial/white_british.txt\" ind=$(basename \"$ind_path\") cov_path=\"/gwas_tutorial/covariates.txt\" cov=$(basename \"$cov_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p plink_gwas_$pheno dx cd plink_gwas_$pheno for chr_num in $(seq 1 22); do prefix=\"/Bulk/DRAGEN\\ WGS/DRAGEN\\ population\\ level\\ WGS\\ variants,\\ PLINK\\ format\\ [500k\\ release]//ukb24308_c${chr_num}_b0_v1\" pfile=$(basename \"$prefix\") plink_command=\"plink2 \\ --threads $threads \\ --maf 0.01 \\ --hwe 1e-50 \\ --geno 0.1 \\ --glm 'qt-residualize' 'hide-covar' \\ --keep $ind \\ --covar $cov \\ --covar-name PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,PC11,PC12,PC13,PC14,PC15,PC16,Age,Sex \\ --pheno $pheno.txt \\ --pfile $pfile \\ --no-psam-pheno \\ --no-input-missing-phenotype \\ --out sumstat_c${chr_num}\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$plink_command\" \\ --instance-type \"$instance\" \\ --name=\"plink_gwas_${pheno}_c${chr_num}\" \\ --tag=\"plink\" \\ --tag=\"GWAS\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$ind_path\" \\ -iin=\"$cov_path\" \\ -iin=\"$pheno_path\" \\ -iin=\"$prefix.pgen\" \\ -iin=\"$prefix.psam\" \\ -iin=\"$prefix.pvar\" \\ -y done dx cd ../ This command outputs 22 files: sumstat_c<chrom-number>.BMI.glm.linear (1.26 GiB total) contains the values for the regression per chromosome The files will be stored in the main directory, plink_gwas_BMI . Please note, the commands are the same whether your phenotype is quantitative or binary. Only the name of the output file will change ( .glm.linear or .glm.logistic.hybrid for linear and logistic regression respectively). Computing the results Now that all of the summary statistics are computed, we can download them and combine them into one single file. By default, PLINK2 outputs a value for each of the covariates, in addition to the global p-value. However, the hide-covar option we have used removes covariate-specific lines from the main report. Therefore, we only need to concatenate the results into a single file. Please change the value of type based on the regression performed: \" linear \" or \" logistic.hybrid \". pheno=\"BMI\" type=\"linear\" # to change accordingly results_path=\"plink_gwas_$pheno\" stat_path=\"plink_statistics_$pheno\" mkdir -p $stat_path for chr_num in $(seq 1 22); do result=\"sumstat_c${chr_num}.$pheno.glm.$type\" dx download \"$results_path/$result\" -o $stat_path if [ $chr_num -eq 1 ]; then head -n1 \"$stat_path/$result\" > \"plink_sumstat_${pheno}.tsv\" fi tail -n +2 \"$stat_path/$result\" >> \"plink_sumstat_${pheno}.tsv\" done This command outputs 23 files locally : sumstat_c<chrom-number>.glm.linear (1.23 Go total) contains the values for the regression per chromosome plink_sumstat_BMI.tsv (1.2 Go) contains the concatenated values for the regression The files will be stored in a new directory named plink_statistics_BMI , locally this time, containing all of the summary statistics per chromosome. The combination of all of them will be located outside plink_statistics_BMI , at the same level, making it easier to find. Although the result files are big, their download should only take about a minute. Congratulations, you have successfully completed a GWAS using PLINK2 on DNAnexus!","title":"Using PLINK2"},{"location":"plink/#using-plink2","text":"Every file created during this analysis will be stored in a main directory called plink_gwas_BMI . This section will run 22 different jobs (one per chromosome). With our chosen instance (mem1_ssd1_v2_x16) using a high priority, the whole GWAS will take about 40 minutes and cost around \u00a33.84 (for a total execution time of 528 minutes). With the same instance (mem1_ssd1_v2_x16) using a low priority, if no jobs are interrupted, it will cost only \u00a31.03 for the same time. Please note that it is most unlikely for jobs to be uninterrupted. In our experience, when accounting for interrupted/failed jobs, the total cost is around one to two times higher than the low cost, but still lower than the high cost. Although please keep in mind that the total time needed can be two to five times higher, depending on the interrupted/failed jobs.","title":"Using PLINK2"},{"location":"plink/#input-files","text":"The path to the genetic data chosen is the following: /Bulk/DRAGEN WGS/DRAGEN population level WGS variants, PLINK format [500k release]/ . Before running a GWAS on DNAnexus using PLINK2, you need to make sure you have these 3 files uploaded to DNAnexus: The phenotype: BMI.txt The ids of individuals we wish to keep: white_british.txt The covariates to use: covariates.txt You can check their presence with the following command: dx ls Please refer to the Input files section if you don't have these files.","title":"Input files"},{"location":"plink/#running-a-gwas","text":"On DNAnexus, PLINK2 is available either as part of the Swiss Army Knife app ( swiss-army-knife ) or as its own app called plink_gwas . We will use the former. We choose to use the same instance for all GWASs, to simplify the code, but this can be changed to your liking. Same for the priority and the cost limit.","title":"Running a GWAS"},{"location":"plink/#quality-control","text":"We perform the QC at the same time as our GWAS. The variants are filtered using the following options: --maf 0.01 --hwe 1e-50 --geno 0.1 Please change the thresholds according to your preferences, but be aware any change will modify jobs execution time, cost and size. Please note that we choose not to use the --mind filter, although it is a conventional, as it removes 88% of the samples for chromosome 18. However, we have not checked for this behavior in other data format so it might be specific to the PLINK2 format.","title":"Quality control"},{"location":"plink/#linear-regression","text":"Following a study by Rivas and Chang in 2025 , we will add the qt-residualize modifier to the --glm option, which performs a single regression on the covariates upfront, and then passes the phenotype residuals. In our experience, this divides the execution time by 3 while yielding the same results as --glm alone, which is why we choose to use it. It also has the added bonus that it scales greatly when analyzing multiple phenotypes (keeping the same execution time for more than a hundred phenotypes). Please be aware that this option may have limits. We recommend making sure that the results are expected. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" ind_path=\"/gwas_tutorial/white_british.txt\" ind=$(basename \"$ind_path\") cov_path=\"/gwas_tutorial/covariates.txt\" cov=$(basename \"$cov_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p plink_gwas_$pheno dx cd plink_gwas_$pheno for chr_num in $(seq 1 22); do prefix=\"/Bulk/DRAGEN\\ WGS/DRAGEN\\ population\\ level\\ WGS\\ variants,\\ PLINK\\ format\\ [500k\\ release]//ukb24308_c${chr_num}_b0_v1\" pfile=$(basename \"$prefix\") plink_command=\"plink2 \\ --threads $threads \\ --maf 0.01 \\ --hwe 1e-50 \\ --geno 0.1 \\ --glm 'qt-residualize' 'hide-covar' \\ --keep $ind \\ --covar $cov \\ --covar-name PC1,PC2,PC3,PC4,PC5,PC6,PC7,PC8,PC9,PC10,PC11,PC12,PC13,PC14,PC15,PC16,Age,Sex \\ --pheno $pheno.txt \\ --pfile $pfile \\ --no-psam-pheno \\ --no-input-missing-phenotype \\ --out sumstat_c${chr_num}\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$plink_command\" \\ --instance-type \"$instance\" \\ --name=\"plink_gwas_${pheno}_c${chr_num}\" \\ --tag=\"plink\" \\ --tag=\"GWAS\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$ind_path\" \\ -iin=\"$cov_path\" \\ -iin=\"$pheno_path\" \\ -iin=\"$prefix.pgen\" \\ -iin=\"$prefix.psam\" \\ -iin=\"$prefix.pvar\" \\ -y done dx cd ../ This command outputs 22 files: sumstat_c<chrom-number>.BMI.glm.linear (1.26 GiB total) contains the values for the regression per chromosome The files will be stored in the main directory, plink_gwas_BMI . Please note, the commands are the same whether your phenotype is quantitative or binary. Only the name of the output file will change ( .glm.linear or .glm.logistic.hybrid for linear and logistic regression respectively).","title":"Linear regression"},{"location":"plink/#computing-the-results","text":"Now that all of the summary statistics are computed, we can download them and combine them into one single file. By default, PLINK2 outputs a value for each of the covariates, in addition to the global p-value. However, the hide-covar option we have used removes covariate-specific lines from the main report. Therefore, we only need to concatenate the results into a single file. Please change the value of type based on the regression performed: \" linear \" or \" logistic.hybrid \". pheno=\"BMI\" type=\"linear\" # to change accordingly results_path=\"plink_gwas_$pheno\" stat_path=\"plink_statistics_$pheno\" mkdir -p $stat_path for chr_num in $(seq 1 22); do result=\"sumstat_c${chr_num}.$pheno.glm.$type\" dx download \"$results_path/$result\" -o $stat_path if [ $chr_num -eq 1 ]; then head -n1 \"$stat_path/$result\" > \"plink_sumstat_${pheno}.tsv\" fi tail -n +2 \"$stat_path/$result\" >> \"plink_sumstat_${pheno}.tsv\" done This command outputs 23 files locally : sumstat_c<chrom-number>.glm.linear (1.23 Go total) contains the values for the regression per chromosome plink_sumstat_BMI.tsv (1.2 Go) contains the concatenated values for the regression The files will be stored in a new directory named plink_statistics_BMI , locally this time, containing all of the summary statistics per chromosome. The combination of all of them will be located outside plink_statistics_BMI , at the same level, making it easier to find. Although the result files are big, their download should only take about a minute. Congratulations, you have successfully completed a GWAS using PLINK2 on DNAnexus!","title":"Computing the results"},{"location":"regenie/","text":"Using regenie Every file created during this analysis will be stored in a main directory called regenie_gwas_BMI . This section runs a total of 47 different jobs, over four stages: Quality control runs 22 jobs (one per chromosome) Merging files runs 2 jobs, for the merge and the QC of the merged data Step 1 runs 1 job for regenie's SNPs contribution estimation (step 1) Step 2 runs 22 job for regenie's regression (step 2, one per chromosome) To optimize your GWAS's overall time, you can perform the QC in parallel to every other step beside step 2 (for which it is needed). The rest needs to be done in order. With our chosen instance (mem1_ssd1_v2_x16) using a high priority, the whole GWAS will take about 830 minutes (13h50, when running each step in order) and cost around \u00a315.89 (for a total execution time of 2183 minutes). With the same instance (mem1_ssd1_v2_x16) using a low priority, if no jobs are interrupted, it will cost only \u00a34.25 for the same time. You can find the details about the jobs run in the following table: Action Time Execution time Cost (high) Cost (low, no interruption) QC 1h41 20h 8.77 2.35 Merge 46min 46min 0.33 0.09 Merge QC 12min 12min 0.09 0.02 Step 1 10h55 10h55 4.77 1.28 Step 2 20min 4h25 1.93 0.52 Please note that it is most unlikely for jobs to be uninterrupted. In our experience, when accounting for interrupted/failed jobs, the total cost is around one to two times higher than the low cost, but still lower than the high cost. Although please keep in mind that the total time needed can be two to five times higher, depending on the interrupted/failed jobs. Input files The path to the genetic data chosen is the following: /Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]/ . Before running a GWAS on DNAnexus using regenie, you need to make sure you have these 3 files uploaded to DNAnexus: The phenotype: BMI.txt The ids of individuals we wish to keep: white_british.txt The covariates to use: covariates.txt You can check their presence with the following command: dx ls Please refer to the Input files section if you don't have these files. Running a GWAS On DNAnexus, regenie is available either as part of the Swiss Army Knife app ( swiss-army-knife ) or as its own app called regenie . We will use the former. We choose to use the same instance for all GWASs, to simplify the code, but this can be changed to your liking. Same for the priority and the cost limit. There is one exception: Step 1 for which we recommend using a high priority. Quality control Unlike with PLINK2, we cannot perform the QC at the same time as our GWAS, we must do it before hand, in preparation for running Step 2 . The variants are filtered using the following options: --maf 0.01 --hwe 1e-50 --geno 0.1 --mind 0.1 Please change the thresholds according to your preferences, but be aware any change will modify jobs execution time, cost and size. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" ind_path=\"/gwas_tutorial/white_british.txt\" ind=$(basename \"$ind_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno/QC_lists dx cd regenie_gwas_$pheno/QC_lists for chr_num in $(seq 1 22); do prefix=\"/Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]//ukb24306_c${chr_num}_b0_v1\" bgen=$(basename \"$prefix\") plink_command=\"plink2 \\ --threads $threads \\ --maf 0.01 \\ --hwe 1e-50 \\ --geno 0.1 \\ --mind 0.1 \\ --write-snplist allow-dups \\ --write-samples \\ --no-id-header \\ --keep $ind \\ --bgen $bgen.bgen ref-unknown \\ --sample $bgen.sample \\ --pheno $pheno.txt \\ --no-psam-pheno \\ --no-input-missing-phenotype \\ --out QC_pass_c${chr_num}\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$plink_command\" \\ --instance-type $instance \\ --name=\"regenie_QC_${pheno}_c${chr_num}\" \\ --tag=\"regenie\" \\ --tag=\"QC\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$ind_path\" \\ -iin=\"$pheno_path\" \\ -iin=\"$prefix.bgen\" \\ -iin=\"$prefix.sample\" \\ -y done dx cd ../../ This command outputs 44 files: QC_pass_c<chrom-number>.snplist (123.08 MiB total) contains a list of SNPs that pass QC per chromosome QC_pass_c<chrom-number>.id (58.96 MiB total) contains a list of sample IDs that pass QC per chromosome They will be stored into another directory named QC_lists to avoid crowding the main repertory for the GWAS. Merging files Before running our GWAS using regenie, we first need to merge all of the genotype call files (chromosome 1 to 22) into one file. This is in preparation for running Step 1 . pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" # not strictly needed, but swiss-army-knife needs at least one input prefix=\"/mnt/project/Bulk/Genotype\\ Results/Genotype\\ calls/\" geno_array=\"ukb22418_c[1-9]*\" instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno/merge dx cd regenie_gwas_$pheno/merge merge_cmd=\"cp $prefix$geno_array . ; \\ ls *.bed | sed -e 's/.bed//g' > files_to_merge.txt; \\ plink \\ --merge-list files_to_merge.txt \\ --make-bed \\ --autosome \\ --out c1_c22_merged; \\ rm files_to_merge.txt; rm $geno_array;\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$merge_cmd\" \\ --instance-type $instance \\ --name=\"regenie_merge\" \\ --tag=\"regenie\" \\ --tag=\"Merge\" \\ -iin=\"$pheno_path\" \\ -y dx cd ../../ This command outputs 3 files: c1_c22_merged.bed (89.18 GiB) contains the genotype table for our merged array genotype data c1_c22_merged.bim (21.47 MiB) contains extended variant information for our merged array genotype data c1_c22_merged.fam (11.64 MiB) contains the sample information for our merged array genotype data The files will be stored in a new directory named merge . Now we can perform QC on this data as well. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" ind_path=\"/gwas_tutorial/white_british.txt\" ind=$(basename \"$ind_path\") merge_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/c1_c22_merged\" merge=$(basename \"$merge_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno/merge dx cd regenie_gwas_$pheno/merge plink_command=\"plink2 \\ --threads $threads \\ --maf 0.01 \\ --hwe 1e-50 \\ --geno 0.1 \\ --mind 0.1 \\ --write-snplist \\ --write-samples \\ --no-id-header \\ --keep $ind \\ --bfile $merge \\ --pheno $pheno.txt \\ --no-psam-pheno \\ --out QC_pass_geno_array\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$plink_command\" \\ --instance-type $instance \\ --name=\"regenie_merge_QC\" \\ --tag=\"regenie\" \\ --tag=\"Merge\" \\ --tag=\"QC\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$ind_path\" \\ -iin=\"$pheno_path\" \\ -iin=\"$merge_path.bed\" \\ -iin=\"$merge_path.bim\" \\ -iin=\"$merge_path.fam\" \\ -y dx cd ../../ This command outputs 2 files: QC_pass_geno_array.snplist (6.14 MiB) contains a list of SNPs that pass QC QC_pass_geno_array.id (6.57 MiB) contains a list of sample IDs that pass QC Like in the QC step , we need to save both the list of SNPs and the list of sample IDs that pass QC for our array genotype data. They are stored in the merge directory. Step 1: Estimate SNPs contribution The first step of a regenie GWAS is the estimation of how background SNPs contribute to the phenotype. During this step, a subset of genetic markers are used to fit a whole genome regression model that captures a good fraction of the phenotype variance attributable to genetic effects. For more information, check the official documentation . We expect this step to take around 10h, which is quite a lot to be uninterrupted (although it is possible). Therefore, we choose to use a high priority, to ensure the job's completion. Please be aware that this step can be common to all GWAS you run with the same QC for the merging step. To not require too much space, we gzip the results using the --gz option. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" cov_path=\"/gwas_tutorial/covariates.txt\" cov=$(basename \"$cov_path\") merge_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/c1_c22_merged\" merge=$(basename \"$merge_path\") QC_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/QC_pass_geno_array\" QC=$(basename \"$QC_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"high\" cost_limit=5 dx mkdir -p regenie_gwas_$pheno/merge dx cd regenie_gwas_$pheno/merge regenie_command=\"regenie \\ --threads $threads \\ --step 1 \\ --bsize 1000 \\ --loocv \\ --gz \\ --extract $QC.snplist \\ --keep $QC.id \\ --bed $merge \\ --phenoFile $pheno.txt \\ --phenoCol $pheno \\ --covarFile $cov \\ --covarCol PC{1:16} \\ --covarCol Sex \\ --covarCol Age \\ --out ${pheno}_merged\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$regenie_command\" \\ --instance-type $instance \\ --name=\"regenie_step1_${pheno}\" \\ --tag=\"regenie\" \\ --tag=\"Step 1\" \\ --tag=\"$pheno\" \\ -iin=\"$pheno_path\" \\ -iin=\"$cov_path\" \\ -iin=\"$merge_path.bed\" \\ -iin=\"$merge_path.bim\" \\ -iin=\"$merge_path.fam\" \\ -iin=\"$QC_path.snplist\" \\ -iin=\"$QC_path.id\" \\ -y dx cd ../../ This command outputs 2 files: BMI_merged_pred.list (48 B) contains a list of blup files needed for Step 2 BMI_merged_1.loco.gz (38.29 MiB) contains per-chromosome LOCO predictions Please note, when using a binary phenotype you need to add the --bt option to the regenie command. Step 2: Linear regression The second step of a regenie GWAS is the regression. During this step, whole genome markers are tested for association with the phenotype conditional upon the prediction from the regression model in Step 1 . For more information, check the official documentation . To not require too much space, we gzip the results using the --gz option. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" cov_path=\"/gwas_tutorial/covariates.txt\" cov=$(basename \"$cov_path\") pred_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/${pheno}_merged_pred.list\" pred=$(basename \"$pred_path\") loco_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/${pheno}_merged_1.loco.gz\" instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno dx cd regenie_gwas_$pheno for chr_num in $(seq 1 22); do prefix=\"/Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]//ukb24306_c${chr_num}_b0_v1\" bgen=$(basename \"$prefix\") QC_path=\"/gwas_tutorial/regenie_gwas_$pheno/QC_lists/QC_pass_c${chr_num}\" QC=$(basename \"$QC_path\") regenie_command=\"regenie \\ --threads $threads \\ --step 2 \\ --bsize 200 \\ --approx \\ --firth-se \\ --firth \\ --gz \\ --pred $pred \\ --extract $QC.snplist \\ --keep $QC.id \\ --bgen $bgen.bgen \\ --sample $bgen.sample \\ --phenoFile $pheno.txt \\ --phenoCol $pheno \\ --covarFile $cov \\ --covarCol PC{1:16} \\ --covarCol Sex \\ --covarCol Age \\ --out sumstat_c${chr_num};\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$regenie_command\" \\ --instance-type $instance \\ --name=\"regenie_step2_${pheno}_c${chr_num}\" \\ --tag=\"regenie\" \\ --tag=\"Step 2\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$pheno_path\" \\ -iin=\"$cov_path\" \\ -iin=\"$prefix.bgen\" \\ -iin=\"$prefix.sample\" \\ -iin=\"$pred_path\" \\ -iin=\"$loco_path\" \\ -iin=\"$QC_path.snplist\" \\ -iin=\"$QC_path.id\" \\ -y done dx cd ../ This command outputs 22 files: sumstat_c<chrom-number>_BMI.regenie.gz (201.1 MiB total) contains the values for the regression per chromosome The files will be stored in the main directory, regenie_gwas_BMI . Please note, when using a binary phenotype you need to add the --bt option to the regenie command to perform a logistic regression rather than a linear one. Computing the results Now that all of the summary statistics are computed, we can download them and combine them into one single file. pheno=\"BMI\" results_path=\"regenie_gwas_$pheno\" stat_path=\"regenie_statistics_$pheno\" mkdir -p $stat_path for chr_num in $(seq 1 22); do result=\"sumstat_c${chr_num}_$pheno.regenie\" dx download \"$results_path/$result.gz\" -o $stat_path gunzip $stat_path/$result if [ $chr_num -eq 1 ]; then head -n1 \"$stat_path/$result\" > \"regenie_sumstat_${pheno}.tsv\" fi tail -n +2 \"$stat_path/$result\" >> \"regenie_sumstat_${pheno}.tsv\" done This command outputs 23 files locally : sumstat_c<chrom-number>.regenie (707.17 Mo total) contains the values for the regression per chromosome regenie_sumstat_BMI.tsv (706.9 Mo) contains the concatenated values for the regression The files will be stored in a new directory named regenie_statistics_BMI , locally this time, containing all of the summary statistics per chromosome. The combination of all of them will be located at the same level as regenie_statistics_BMI , making it easier to find. Although the result files are quite big, their download should only take about a minute. Congratulations, you have successfully completed a GWAS using regenie on DNAnexus!","title":"Using regenie"},{"location":"regenie/#using-regenie","text":"Every file created during this analysis will be stored in a main directory called regenie_gwas_BMI . This section runs a total of 47 different jobs, over four stages: Quality control runs 22 jobs (one per chromosome) Merging files runs 2 jobs, for the merge and the QC of the merged data Step 1 runs 1 job for regenie's SNPs contribution estimation (step 1) Step 2 runs 22 job for regenie's regression (step 2, one per chromosome) To optimize your GWAS's overall time, you can perform the QC in parallel to every other step beside step 2 (for which it is needed). The rest needs to be done in order. With our chosen instance (mem1_ssd1_v2_x16) using a high priority, the whole GWAS will take about 830 minutes (13h50, when running each step in order) and cost around \u00a315.89 (for a total execution time of 2183 minutes). With the same instance (mem1_ssd1_v2_x16) using a low priority, if no jobs are interrupted, it will cost only \u00a34.25 for the same time. You can find the details about the jobs run in the following table: Action Time Execution time Cost (high) Cost (low, no interruption) QC 1h41 20h 8.77 2.35 Merge 46min 46min 0.33 0.09 Merge QC 12min 12min 0.09 0.02 Step 1 10h55 10h55 4.77 1.28 Step 2 20min 4h25 1.93 0.52 Please note that it is most unlikely for jobs to be uninterrupted. In our experience, when accounting for interrupted/failed jobs, the total cost is around one to two times higher than the low cost, but still lower than the high cost. Although please keep in mind that the total time needed can be two to five times higher, depending on the interrupted/failed jobs.","title":"Using regenie"},{"location":"regenie/#input-files","text":"The path to the genetic data chosen is the following: /Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]/ . Before running a GWAS on DNAnexus using regenie, you need to make sure you have these 3 files uploaded to DNAnexus: The phenotype: BMI.txt The ids of individuals we wish to keep: white_british.txt The covariates to use: covariates.txt You can check their presence with the following command: dx ls Please refer to the Input files section if you don't have these files.","title":"Input files"},{"location":"regenie/#running-a-gwas","text":"On DNAnexus, regenie is available either as part of the Swiss Army Knife app ( swiss-army-knife ) or as its own app called regenie . We will use the former. We choose to use the same instance for all GWASs, to simplify the code, but this can be changed to your liking. Same for the priority and the cost limit. There is one exception: Step 1 for which we recommend using a high priority.","title":"Running a GWAS"},{"location":"regenie/#quality-control","text":"Unlike with PLINK2, we cannot perform the QC at the same time as our GWAS, we must do it before hand, in preparation for running Step 2 . The variants are filtered using the following options: --maf 0.01 --hwe 1e-50 --geno 0.1 --mind 0.1 Please change the thresholds according to your preferences, but be aware any change will modify jobs execution time, cost and size. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" ind_path=\"/gwas_tutorial/white_british.txt\" ind=$(basename \"$ind_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno/QC_lists dx cd regenie_gwas_$pheno/QC_lists for chr_num in $(seq 1 22); do prefix=\"/Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]//ukb24306_c${chr_num}_b0_v1\" bgen=$(basename \"$prefix\") plink_command=\"plink2 \\ --threads $threads \\ --maf 0.01 \\ --hwe 1e-50 \\ --geno 0.1 \\ --mind 0.1 \\ --write-snplist allow-dups \\ --write-samples \\ --no-id-header \\ --keep $ind \\ --bgen $bgen.bgen ref-unknown \\ --sample $bgen.sample \\ --pheno $pheno.txt \\ --no-psam-pheno \\ --no-input-missing-phenotype \\ --out QC_pass_c${chr_num}\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$plink_command\" \\ --instance-type $instance \\ --name=\"regenie_QC_${pheno}_c${chr_num}\" \\ --tag=\"regenie\" \\ --tag=\"QC\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$ind_path\" \\ -iin=\"$pheno_path\" \\ -iin=\"$prefix.bgen\" \\ -iin=\"$prefix.sample\" \\ -y done dx cd ../../ This command outputs 44 files: QC_pass_c<chrom-number>.snplist (123.08 MiB total) contains a list of SNPs that pass QC per chromosome QC_pass_c<chrom-number>.id (58.96 MiB total) contains a list of sample IDs that pass QC per chromosome They will be stored into another directory named QC_lists to avoid crowding the main repertory for the GWAS.","title":"Quality control"},{"location":"regenie/#merging-files","text":"Before running our GWAS using regenie, we first need to merge all of the genotype call files (chromosome 1 to 22) into one file. This is in preparation for running Step 1 . pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" # not strictly needed, but swiss-army-knife needs at least one input prefix=\"/mnt/project/Bulk/Genotype\\ Results/Genotype\\ calls/\" geno_array=\"ukb22418_c[1-9]*\" instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno/merge dx cd regenie_gwas_$pheno/merge merge_cmd=\"cp $prefix$geno_array . ; \\ ls *.bed | sed -e 's/.bed//g' > files_to_merge.txt; \\ plink \\ --merge-list files_to_merge.txt \\ --make-bed \\ --autosome \\ --out c1_c22_merged; \\ rm files_to_merge.txt; rm $geno_array;\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$merge_cmd\" \\ --instance-type $instance \\ --name=\"regenie_merge\" \\ --tag=\"regenie\" \\ --tag=\"Merge\" \\ -iin=\"$pheno_path\" \\ -y dx cd ../../ This command outputs 3 files: c1_c22_merged.bed (89.18 GiB) contains the genotype table for our merged array genotype data c1_c22_merged.bim (21.47 MiB) contains extended variant information for our merged array genotype data c1_c22_merged.fam (11.64 MiB) contains the sample information for our merged array genotype data The files will be stored in a new directory named merge . Now we can perform QC on this data as well. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" ind_path=\"/gwas_tutorial/white_british.txt\" ind=$(basename \"$ind_path\") merge_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/c1_c22_merged\" merge=$(basename \"$merge_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno/merge dx cd regenie_gwas_$pheno/merge plink_command=\"plink2 \\ --threads $threads \\ --maf 0.01 \\ --hwe 1e-50 \\ --geno 0.1 \\ --mind 0.1 \\ --write-snplist \\ --write-samples \\ --no-id-header \\ --keep $ind \\ --bfile $merge \\ --pheno $pheno.txt \\ --no-psam-pheno \\ --out QC_pass_geno_array\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$plink_command\" \\ --instance-type $instance \\ --name=\"regenie_merge_QC\" \\ --tag=\"regenie\" \\ --tag=\"Merge\" \\ --tag=\"QC\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$ind_path\" \\ -iin=\"$pheno_path\" \\ -iin=\"$merge_path.bed\" \\ -iin=\"$merge_path.bim\" \\ -iin=\"$merge_path.fam\" \\ -y dx cd ../../ This command outputs 2 files: QC_pass_geno_array.snplist (6.14 MiB) contains a list of SNPs that pass QC QC_pass_geno_array.id (6.57 MiB) contains a list of sample IDs that pass QC Like in the QC step , we need to save both the list of SNPs and the list of sample IDs that pass QC for our array genotype data. They are stored in the merge directory.","title":"Merging files"},{"location":"regenie/#step-1-estimate-snps-contribution","text":"The first step of a regenie GWAS is the estimation of how background SNPs contribute to the phenotype. During this step, a subset of genetic markers are used to fit a whole genome regression model that captures a good fraction of the phenotype variance attributable to genetic effects. For more information, check the official documentation . We expect this step to take around 10h, which is quite a lot to be uninterrupted (although it is possible). Therefore, we choose to use a high priority, to ensure the job's completion. Please be aware that this step can be common to all GWAS you run with the same QC for the merging step. To not require too much space, we gzip the results using the --gz option. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" cov_path=\"/gwas_tutorial/covariates.txt\" cov=$(basename \"$cov_path\") merge_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/c1_c22_merged\" merge=$(basename \"$merge_path\") QC_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/QC_pass_geno_array\" QC=$(basename \"$QC_path\") instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"high\" cost_limit=5 dx mkdir -p regenie_gwas_$pheno/merge dx cd regenie_gwas_$pheno/merge regenie_command=\"regenie \\ --threads $threads \\ --step 1 \\ --bsize 1000 \\ --loocv \\ --gz \\ --extract $QC.snplist \\ --keep $QC.id \\ --bed $merge \\ --phenoFile $pheno.txt \\ --phenoCol $pheno \\ --covarFile $cov \\ --covarCol PC{1:16} \\ --covarCol Sex \\ --covarCol Age \\ --out ${pheno}_merged\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$regenie_command\" \\ --instance-type $instance \\ --name=\"regenie_step1_${pheno}\" \\ --tag=\"regenie\" \\ --tag=\"Step 1\" \\ --tag=\"$pheno\" \\ -iin=\"$pheno_path\" \\ -iin=\"$cov_path\" \\ -iin=\"$merge_path.bed\" \\ -iin=\"$merge_path.bim\" \\ -iin=\"$merge_path.fam\" \\ -iin=\"$QC_path.snplist\" \\ -iin=\"$QC_path.id\" \\ -y dx cd ../../ This command outputs 2 files: BMI_merged_pred.list (48 B) contains a list of blup files needed for Step 2 BMI_merged_1.loco.gz (38.29 MiB) contains per-chromosome LOCO predictions Please note, when using a binary phenotype you need to add the --bt option to the regenie command.","title":"Step 1: Estimate SNPs contribution"},{"location":"regenie/#step-2-linear-regression","text":"The second step of a regenie GWAS is the regression. During this step, whole genome markers are tested for association with the phenotype conditional upon the prediction from the regression model in Step 1 . For more information, check the official documentation . To not require too much space, we gzip the results using the --gz option. pheno=\"BMI\" pheno_path=\"/gwas_tutorial/$pheno.txt\" cov_path=\"/gwas_tutorial/covariates.txt\" cov=$(basename \"$cov_path\") pred_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/${pheno}_merged_pred.list\" pred=$(basename \"$pred_path\") loco_path=\"/gwas_tutorial/regenie_gwas_$pheno/merge/${pheno}_merged_1.loco.gz\" instance=\"mem1_ssd1_v2_x16\" threads=16 priority=\"low\" cost_limit=3 dx mkdir -p regenie_gwas_$pheno dx cd regenie_gwas_$pheno for chr_num in $(seq 1 22); do prefix=\"/Bulk/Previous WGS releases/GATK and GraphTyper WGS/GraphTyper population level genome variants, BGEN format [200k release]//ukb24306_c${chr_num}_b0_v1\" bgen=$(basename \"$prefix\") QC_path=\"/gwas_tutorial/regenie_gwas_$pheno/QC_lists/QC_pass_c${chr_num}\" QC=$(basename \"$QC_path\") regenie_command=\"regenie \\ --threads $threads \\ --step 2 \\ --bsize 200 \\ --approx \\ --firth-se \\ --firth \\ --gz \\ --pred $pred \\ --extract $QC.snplist \\ --keep $QC.id \\ --bgen $bgen.bgen \\ --sample $bgen.sample \\ --phenoFile $pheno.txt \\ --phenoCol $pheno \\ --covarFile $cov \\ --covarCol PC{1:16} \\ --covarCol Sex \\ --covarCol Age \\ --out sumstat_c${chr_num};\" dx run swiss-army-knife \\ --priority \"$priority\" --cost-limit \"$cost_limit\" \\ -icmd=\"$regenie_command\" \\ --instance-type $instance \\ --name=\"regenie_step2_${pheno}_c${chr_num}\" \\ --tag=\"regenie\" \\ --tag=\"Step 2\" \\ --tag=\"$pheno\" \\ --tag=\"c${chr_num}\" \\ -iin=\"$pheno_path\" \\ -iin=\"$cov_path\" \\ -iin=\"$prefix.bgen\" \\ -iin=\"$prefix.sample\" \\ -iin=\"$pred_path\" \\ -iin=\"$loco_path\" \\ -iin=\"$QC_path.snplist\" \\ -iin=\"$QC_path.id\" \\ -y done dx cd ../ This command outputs 22 files: sumstat_c<chrom-number>_BMI.regenie.gz (201.1 MiB total) contains the values for the regression per chromosome The files will be stored in the main directory, regenie_gwas_BMI . Please note, when using a binary phenotype you need to add the --bt option to the regenie command to perform a logistic regression rather than a linear one.","title":"Step 2: Linear regression"},{"location":"regenie/#computing-the-results","text":"Now that all of the summary statistics are computed, we can download them and combine them into one single file. pheno=\"BMI\" results_path=\"regenie_gwas_$pheno\" stat_path=\"regenie_statistics_$pheno\" mkdir -p $stat_path for chr_num in $(seq 1 22); do result=\"sumstat_c${chr_num}_$pheno.regenie\" dx download \"$results_path/$result.gz\" -o $stat_path gunzip $stat_path/$result if [ $chr_num -eq 1 ]; then head -n1 \"$stat_path/$result\" > \"regenie_sumstat_${pheno}.tsv\" fi tail -n +2 \"$stat_path/$result\" >> \"regenie_sumstat_${pheno}.tsv\" done This command outputs 23 files locally : sumstat_c<chrom-number>.regenie (707.17 Mo total) contains the values for the regression per chromosome regenie_sumstat_BMI.tsv (706.9 Mo) contains the concatenated values for the regression The files will be stored in a new directory named regenie_statistics_BMI , locally this time, containing all of the summary statistics per chromosome. The combination of all of them will be located at the same level as regenie_statistics_BMI , making it easier to find. Although the result files are quite big, their download should only take about a minute. Congratulations, you have successfully completed a GWAS using regenie on DNAnexus!","title":"Computing the results"},{"location":"results/","text":"Visualizing results Once your GWAS is complete, you might want to analyze its summary statistics. Here, we provide code to compute a Manhattan plot and a QQ plot from your results. Generatating plots We propose the following code, as a way to see how such plots are made. Please keep in mind this code is not really optimized, but shoudl run decently quickly on big data. We suggest using other softwares with which you are more familiar, or that you know are faster. \"\"\"Generate Manhattan plot and QQ plot from GWAS summary statistics.\"\"\" import numpy as np import matplotlib.pyplot as plt def read_results(phenotype, software): \"\"\" Parse GWAS result file and get values. Parameters ---------- phenotype : str Phenotype name. software : str { 'p', 'r'} Either 'p' for PLINK2 or 'r' for regenie, to correctly format the file. Returns ------- list Chromosome numbers. list Transformed p-values (-log10). \"\"\" chroms = [] pvals = [] # Software specific format. if software == 'p': chrom_name = \"#CHROM\" p_name = \"P\" softname = \"plink\" elif software == 'r': chrom_name = \"CHROM\" p_name = \"LOG10P\" softname = \"regenie\" filename = f\"{softname}_sumstat_{phenotype}.tsv\" with open(filename, 'r') as glm: for line in glm: columns = line.split() if line.startswith(chrom_name): chrom_id = columns.index(chrom_name) p_id = columns.index(p_name) else: chroms.append(int(columns[chrom_id])) if software == 'p': pvals.append(-np.log10(float(columns[p_id]))) elif software == 'r': pvals.append(float(columns[p_id])) return chroms, pvals def manhattan_plot(chroms, pvals, phenotype, software): \"\"\" Generate Manhattan plot. Parameters ---------- chroms : list Chromosome numbers. pvals : list Transformed p-values (-log10). phenotype : str Phenotype name. software : str { 'p', 'r'} Either 'p' for PLINK2 or 'r' for regenie, to correctly format the file. Returns ------- img Manhattan plot. \"\"\" # Software specific format. if software == 'p': softname = \"plink\" elif software == 'r': softname = \"regenie\" output = f\"{softname}_{phenotype}_Manhattan_plot.png\" chrom_distrib = {} colors = [] for chrom in chroms: chrom_number = chrom # Specify color if chrom_number%2 == 0: colors.append(\"darkgray\") else: colors.append(\"black\") # Track chromosome number if chrom_number not in chrom_distrib: chrom_distrib[chrom_number] = 1 else: chrom_distrib[chrom_number] += 1 # Plot plt.figure(figsize=(10, 8), dpi=330) plt.scatter(range(len(chroms)), pvals, color=colors, s=5) # Correct x axis istart = 0 iend = 0 x_ticks = [] x_labels = [] for chrom, number in chrom_distrib.items(): # Update end idx iend += number pos = (istart + iend - 1) / 2 x_ticks.append(pos) x_labels.append(chrom) # Update start idx istart = iend # Plot suggestive and significative thresholds plt.axhline(-np.log10(5e-5), linestyle=\"dotted\", color=\"red\") plt.axhline(-np.log10(5e-8), linestyle=\"dashed\", color=\"red\") # Plot info plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90) plt.xlabel(\"Chromosome\") plt.ylabel(\"$-log_{10}(p)$\") # Save plt.savefig(output, bbox_inches=\"tight\") plt.close() def qq_plot(pvals, phenotype, software): \"\"\" Generate QQ plot. Parameters ---------- pvals : list Transformed p-values (-log10). phenotype : str Phenotype name. software : str { 'p', 'r'} Either 'p' for PLINK2 or 'r' for regenie, to correctly format the file. Returns ------- img QQ plot. \"\"\" # Software specific format. if software == 'p': softname = \"plink\" elif software == 'r': softname = \"regenie\" output = f\"{softname}_{phenotype}_QQ_plot.png\" # Sort p-values pvals.sort(reverse=True) # Expected values n = len(pvals) exp = -np.log10(np.arange(n, dtype=float) / n) # Plot plt.figure(figsize=(10, 8), dpi=330) plt.axline((0, 0), slope=1, color=\"red\", linestyle=\"dashed\", zorder=0) plt.scatter(exp, pvals, color=\"black\", s=5, zorder=10) plt.xlabel(\"Expected $-log_{10}(p)$\") plt.ylabel(\"Observed $-log_{10}(p)$\") # Save plt.savefig(output, bbox_inches=\"tight\") plt.close() # Input PHENOTYPE = \"BMI\" SOFTWARE = 'p' # for PLINK2 or 'r' for regenie # Read values CHROMS, PVALS = read_results(PHENOTYPE, SOFTWARE) # Manhattan Plot manhattan_plot(CHROMS, PVALS, PHENOTYPE, SOFTWARE) # QQ plot qq_plot(PVALS, PHENOTYPE, SOFTWARE) This command outputs 2 files: <software>_BMI_Manhattan_plot.png (~190 Ko) contains the Manhattan plot for the whole GWAS <software>_BMI_QQ_plot.png (~ 110 Ko) contains the QQ plot for the whole GWAS This script should take between 5 and 10 minutes to compute both plots. Expected plots PLINK2 By the end of this tutorial, when using PLINK2, you should obtain the following Manhattan plot: And the following QQ plot: regenie By the end of this tutorial, when using regenie, you should obtain the following Manhattan plot: And the following QQ plot:","title":"Visualizing results"},{"location":"results/#visualizing-results","text":"Once your GWAS is complete, you might want to analyze its summary statistics. Here, we provide code to compute a Manhattan plot and a QQ plot from your results.","title":"Visualizing results"},{"location":"results/#generatating-plots","text":"We propose the following code, as a way to see how such plots are made. Please keep in mind this code is not really optimized, but shoudl run decently quickly on big data. We suggest using other softwares with which you are more familiar, or that you know are faster. \"\"\"Generate Manhattan plot and QQ plot from GWAS summary statistics.\"\"\" import numpy as np import matplotlib.pyplot as plt def read_results(phenotype, software): \"\"\" Parse GWAS result file and get values. Parameters ---------- phenotype : str Phenotype name. software : str { 'p', 'r'} Either 'p' for PLINK2 or 'r' for regenie, to correctly format the file. Returns ------- list Chromosome numbers. list Transformed p-values (-log10). \"\"\" chroms = [] pvals = [] # Software specific format. if software == 'p': chrom_name = \"#CHROM\" p_name = \"P\" softname = \"plink\" elif software == 'r': chrom_name = \"CHROM\" p_name = \"LOG10P\" softname = \"regenie\" filename = f\"{softname}_sumstat_{phenotype}.tsv\" with open(filename, 'r') as glm: for line in glm: columns = line.split() if line.startswith(chrom_name): chrom_id = columns.index(chrom_name) p_id = columns.index(p_name) else: chroms.append(int(columns[chrom_id])) if software == 'p': pvals.append(-np.log10(float(columns[p_id]))) elif software == 'r': pvals.append(float(columns[p_id])) return chroms, pvals def manhattan_plot(chroms, pvals, phenotype, software): \"\"\" Generate Manhattan plot. Parameters ---------- chroms : list Chromosome numbers. pvals : list Transformed p-values (-log10). phenotype : str Phenotype name. software : str { 'p', 'r'} Either 'p' for PLINK2 or 'r' for regenie, to correctly format the file. Returns ------- img Manhattan plot. \"\"\" # Software specific format. if software == 'p': softname = \"plink\" elif software == 'r': softname = \"regenie\" output = f\"{softname}_{phenotype}_Manhattan_plot.png\" chrom_distrib = {} colors = [] for chrom in chroms: chrom_number = chrom # Specify color if chrom_number%2 == 0: colors.append(\"darkgray\") else: colors.append(\"black\") # Track chromosome number if chrom_number not in chrom_distrib: chrom_distrib[chrom_number] = 1 else: chrom_distrib[chrom_number] += 1 # Plot plt.figure(figsize=(10, 8), dpi=330) plt.scatter(range(len(chroms)), pvals, color=colors, s=5) # Correct x axis istart = 0 iend = 0 x_ticks = [] x_labels = [] for chrom, number in chrom_distrib.items(): # Update end idx iend += number pos = (istart + iend - 1) / 2 x_ticks.append(pos) x_labels.append(chrom) # Update start idx istart = iend # Plot suggestive and significative thresholds plt.axhline(-np.log10(5e-5), linestyle=\"dotted\", color=\"red\") plt.axhline(-np.log10(5e-8), linestyle=\"dashed\", color=\"red\") # Plot info plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90) plt.xlabel(\"Chromosome\") plt.ylabel(\"$-log_{10}(p)$\") # Save plt.savefig(output, bbox_inches=\"tight\") plt.close() def qq_plot(pvals, phenotype, software): \"\"\" Generate QQ plot. Parameters ---------- pvals : list Transformed p-values (-log10). phenotype : str Phenotype name. software : str { 'p', 'r'} Either 'p' for PLINK2 or 'r' for regenie, to correctly format the file. Returns ------- img QQ plot. \"\"\" # Software specific format. if software == 'p': softname = \"plink\" elif software == 'r': softname = \"regenie\" output = f\"{softname}_{phenotype}_QQ_plot.png\" # Sort p-values pvals.sort(reverse=True) # Expected values n = len(pvals) exp = -np.log10(np.arange(n, dtype=float) / n) # Plot plt.figure(figsize=(10, 8), dpi=330) plt.axline((0, 0), slope=1, color=\"red\", linestyle=\"dashed\", zorder=0) plt.scatter(exp, pvals, color=\"black\", s=5, zorder=10) plt.xlabel(\"Expected $-log_{10}(p)$\") plt.ylabel(\"Observed $-log_{10}(p)$\") # Save plt.savefig(output, bbox_inches=\"tight\") plt.close() # Input PHENOTYPE = \"BMI\" SOFTWARE = 'p' # for PLINK2 or 'r' for regenie # Read values CHROMS, PVALS = read_results(PHENOTYPE, SOFTWARE) # Manhattan Plot manhattan_plot(CHROMS, PVALS, PHENOTYPE, SOFTWARE) # QQ plot qq_plot(PVALS, PHENOTYPE, SOFTWARE) This command outputs 2 files: <software>_BMI_Manhattan_plot.png (~190 Ko) contains the Manhattan plot for the whole GWAS <software>_BMI_QQ_plot.png (~ 110 Ko) contains the QQ plot for the whole GWAS This script should take between 5 and 10 minutes to compute both plots.","title":"Generatating plots"},{"location":"results/#expected-plots","text":"","title":"Expected plots"},{"location":"results/#plink2","text":"By the end of this tutorial, when using PLINK2, you should obtain the following Manhattan plot: And the following QQ plot:","title":"PLINK2"},{"location":"results/#regenie","text":"By the end of this tutorial, when using regenie, you should obtain the following Manhattan plot: And the following QQ plot:","title":"regenie"},{"location":"start/","text":"Getting started Login In order to connect to DNAnexus remotely, you first need to install the dxpy package. pip install dxpy To enable tab completion, run the following command, or add it to your .bashrc : eval \"$(register-python-argcomplete dx|sed 's/-o default//')\" You can now enter your DNAnexus credentials to access your project remotely by using the following command: dx login Your authentication token and your current project settings have now been saved in a local configuration file, and you're ready to start accessing your project. By default, your information expires in 30 days, but this can be changed using the --timeout option. For instance, if you want your info to expire in 6 months , use the following command. The -help option is useful if you want to know more about the --timeout input format. dx login --timeout 6M If you have access to several projects on DNAnexus, you need to choose the specific one in which you want to perform your GWAS. Please use the dx select command or check the official documentation on Project Navigation for more information. If you have access to only one project, it will already be selected and you can go on with this tutorial. Architecture When using the command line, you have access to the architecture on DNAnexus and your own local architecture. The basic commands like ls , cd or mv all have equivalents to run on the DNAnexus architecture rather than the local one, simply add dx before them. A quick way to check the difference is to run the same command on both architecture: ls will lists the files in your current directory , as you probably already know dx ls will lists the files in your DNAnexus current directory , which should be the root of your project if you are first starting (with the Bulk and Showcase metadata folders) You can find the official documentation for all dx commands we will use here . By default, any job run from the command line will output in your current DNAnexus repertory, meaning at the root of your project. To keep your project tidy, we can create a new folder and move into it, so that all jobs will output there. dx mkdir gwas_tutorial dx cd gwas_tutorial If multiple people have access and frequently run jobs on the project chosen, we recommend having your own directory like WKD_<your-name> in which you will then create the gwas_tutorial directory. First job If this is your first time using DNAnexus altogether, please go to the following page titled About jobs . It will take you through the fondamentals of jobs on DNAnexus, and will help you understand how and why the code works. If you are already familiar with how DNAnexus jobs work, please skip ahead to the Input files page , which will start the GWAS tutorial.","title":"Getting started"},{"location":"start/#getting-started","text":"","title":"Getting started"},{"location":"start/#login","text":"In order to connect to DNAnexus remotely, you first need to install the dxpy package. pip install dxpy To enable tab completion, run the following command, or add it to your .bashrc : eval \"$(register-python-argcomplete dx|sed 's/-o default//')\" You can now enter your DNAnexus credentials to access your project remotely by using the following command: dx login Your authentication token and your current project settings have now been saved in a local configuration file, and you're ready to start accessing your project. By default, your information expires in 30 days, but this can be changed using the --timeout option. For instance, if you want your info to expire in 6 months , use the following command. The -help option is useful if you want to know more about the --timeout input format. dx login --timeout 6M If you have access to several projects on DNAnexus, you need to choose the specific one in which you want to perform your GWAS. Please use the dx select command or check the official documentation on Project Navigation for more information. If you have access to only one project, it will already be selected and you can go on with this tutorial.","title":"Login"},{"location":"start/#architecture","text":"When using the command line, you have access to the architecture on DNAnexus and your own local architecture. The basic commands like ls , cd or mv all have equivalents to run on the DNAnexus architecture rather than the local one, simply add dx before them. A quick way to check the difference is to run the same command on both architecture: ls will lists the files in your current directory , as you probably already know dx ls will lists the files in your DNAnexus current directory , which should be the root of your project if you are first starting (with the Bulk and Showcase metadata folders) You can find the official documentation for all dx commands we will use here . By default, any job run from the command line will output in your current DNAnexus repertory, meaning at the root of your project. To keep your project tidy, we can create a new folder and move into it, so that all jobs will output there. dx mkdir gwas_tutorial dx cd gwas_tutorial If multiple people have access and frequently run jobs on the project chosen, we recommend having your own directory like WKD_<your-name> in which you will then create the gwas_tutorial directory.","title":"Architecture"},{"location":"start/#first-job","text":"If this is your first time using DNAnexus altogether, please go to the following page titled About jobs . It will take you through the fondamentals of jobs on DNAnexus, and will help you understand how and why the code works. If you are already familiar with how DNAnexus jobs work, please skip ahead to the Input files page , which will start the GWAS tutorial.","title":"First job"}]}